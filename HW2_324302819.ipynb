{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31yxAKBQgrhJ",
        "outputId": "98512e09-383b-43da-aa9f-4a2ba3c26e36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Input Table: Raw Term Frequency (TF)\n",
            "**Total Documents (N):** 10\n",
            "**Query:** computer + game + china + software\n",
            "**Vocabulary (10 terms):** ['computer', 'game', 'china', 'software', 'data', 'football', 'games', 'was', 'video', 's']\n",
            "---\n",
            "               computer  game  china  software  data  football  games  was  \\\n",
            "article1.txt          0     1      0         0     0        32      3    1   \n",
            "article3.txt          1     0      0         3     0         0      0    0   \n",
            "article5.txt          1     0      0         0     0         0      0    0   \n",
            "article4.txt          7     0      0         1     0         0      0    0   \n",
            "article2.txt          5    14      0         0     0         0     18    2   \n",
            "article6.txt          0     0      0         4    44         0      0    2   \n",
            "article9.txt          0     0      0         0     0         0      0    1   \n",
            "article8.txt          0     0     14         0     0         0      0    7   \n",
            "article10.txt         0     3      0         0     0         0      1    7   \n",
            "article7.txt          0     0      0         0     0         0      0    0   \n",
            "\n",
            "               video  s  \n",
            "article1.txt       0  1  \n",
            "article3.txt       0  0  \n",
            "article5.txt       1  0  \n",
            "article4.txt       0  1  \n",
            "article2.txt      18  0  \n",
            "article6.txt       0  3  \n",
            "article9.txt       0  6  \n",
            "article8.txt       0  5  \n",
            "article10.txt      1  2  \n",
            "article7.txt       0  0  \n",
            "\n",
            "================================================================================\n",
            "\n",
            "## Intermediate Results: IDF Scores\n",
            "IDF (Inverse Document Frequency): High scores indicate rare terms.\n",
            "---\n",
            "data        2.302585\n",
            "china       2.302585\n",
            "football    2.302585\n",
            "game        1.203973\n",
            "games       1.203973\n",
            "software    1.203973\n",
            "video       1.203973\n",
            "computer    0.916291\n",
            "was         0.510826\n",
            "s           0.510826\n",
            "dtype: float64\n",
            "\n",
            "================================================================================\n",
            "\n",
            "## Final Output Table: Log TF-IDF and Query Score\n",
            "Table is sorted by 'Query_Score' (relevance).\n",
            "---\n",
            "               Query_Score  computer      game     china  software      data  \\\n",
            "article8.txt      6.235516  0.000000  0.000000  6.235516  0.000000  0.000000   \n",
            "article2.txt      4.902191  1.641773  3.260419  0.000000  0.000000  0.000000   \n",
            "article4.txt      2.739903  1.905373  0.000000  0.000000  0.834530  0.000000   \n",
            "article3.txt      2.304185  0.635124  0.000000  0.000000  1.669061  0.000000   \n",
            "article6.txt      1.937719  0.000000  0.000000  0.000000  1.937719  8.765164   \n",
            "article10.txt     1.669061  0.000000  1.669061  0.000000  0.000000  0.000000   \n",
            "article1.txt      0.834530  0.000000  0.834530  0.000000  0.000000  0.000000   \n",
            "article5.txt      0.635124  0.635124  0.000000  0.000000  0.000000  0.000000   \n",
            "article9.txt      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "article7.txt      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "\n",
            "               football     games       was     video         s  \n",
            "article8.txt   0.000000  0.000000  1.062232  0.000000  0.915277  \n",
            "article2.txt   0.000000  3.545024  0.561199  3.545024  0.000000  \n",
            "article4.txt   0.000000  0.000000  0.000000  0.000000  0.354077  \n",
            "article3.txt   0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "article6.txt   0.000000  0.000000  0.561199  0.000000  0.708155  \n",
            "article10.txt  0.000000  0.834530  1.062232  0.834530  0.561199  \n",
            "article1.txt   8.051006  1.669061  0.354077  0.000000  0.354077  \n",
            "article5.txt   0.000000  0.000000  0.000000  0.834530  0.000000  \n",
            "article9.txt   0.000000  0.000000  0.354077  0.000000  0.994021  \n",
            "article7.txt   0.000000  0.000000  0.000000  0.000000  0.000000  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Tuple\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Configuration ---\n",
        "# Set the current directory as the document source\n",
        "DOCUMENTS_DIR = Path('.')\n",
        "# Define the Query and Vocabulary Size limit\n",
        "QUERY_TERMS = [\"computer\", \"game\", \"china\",\"software\"]\n",
        "VOCABULARY_SIZE = 10\n",
        "# ---------------------\n",
        "\n",
        "def load_documents_from_disk(directory: Path) -> Dict[str, str]:\n",
        "    \"\"\"Dynamically finds and loads content from all .txt files in the specified directory.\"\"\"\n",
        "    docs_content = {}\n",
        "\n",
        "    # Iterate through all .txt files in the directory\n",
        "    for file_path in directory.glob('*.txt'):\n",
        "        try:\n",
        "            # Skip files that might be part of an operating system or non-article files\n",
        "            if file_path.name.startswith('article'):\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    docs_content[file_path.name] = f.read()\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading file {file_path.name}: {e}\")\n",
        "\n",
        "    # If fewer than 10 documents are found, adjust the count\n",
        "    if len(docs_content) == 0:\n",
        "        print(\"Error: No .txt files found. Please ensure your files are named 'articleX.txt' and are in the same directory.\")\n",
        "    elif len(docs_content) < 10:\n",
        "        print(f\"Warning: Only {len(docs_content)} documents found. Calculations will use this number (N={len(docs_content)}).\")\n",
        "\n",
        "    return docs_content\n",
        "\n",
        "def preprocess(text: str) -> List[str]:\n",
        "    \"\"\"Tokenizes text to lowercase words, removing punctuation.\"\"\"\n",
        "    # Remove punctuation and convert to lowercase\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text.lower())\n",
        "    # Split by whitespace\n",
        "    return text.split()\n",
        "\n",
        "def build_tf_matrix(docs_content: Dict[str, str], vocabulary_size: int, query_terms: List[str]) -> Tuple[pd.DataFrame, List[str]]:\n",
        "    \"\"\"Generates the raw TF matrix and selects the top 'vocabulary_size' terms, ensuring query terms are included.\"\"\"\n",
        "    doc_tfs = {}\n",
        "\n",
        "    # 1. Calculate raw TF for all words in all documents\n",
        "    for doc_name, content in docs_content.items():\n",
        "        tokens = preprocess(content)\n",
        "        doc_tfs[doc_name] = Counter(tokens)\n",
        "\n",
        "    # 2. Determine the overall vocabulary (top terms)\n",
        "    all_words = Counter()\n",
        "    for tf_counts in doc_tfs.values():\n",
        "        all_words.update(tf_counts)\n",
        "\n",
        "    # Define common stopwords to exclude from the meaningful vocabulary\n",
        "    stopwords = {'the', 'a', 'of', 'and', 'to', 'in', 'is', 'that', 'with', 'or', 'by', 'as', 'it', 'for', 'are', 'which', 'its', 'from', 'on', 'can','an'}\n",
        "\n",
        "    # Get all words sorted by frequency, excluding stopwords\n",
        "    meaningful_words = [word for word, count in all_words.most_common() if word not in stopwords and word not in query_terms]\n",
        "\n",
        "    # Final Vocabulary: Query terms + top N-K meaningful words (where K is number of query terms)\n",
        "    num_other_words = vocabulary_size - len(query_terms)\n",
        "    final_vocab = query_terms + meaningful_words[:num_other_words]\n",
        "\n",
        "    # 3. Build the final TF DataFrame\n",
        "    df_tf_data = {}\n",
        "    for term in final_vocab:\n",
        "        df_tf_data[term] = [doc_tfs[doc_name].get(term, 0) for doc_name in docs_content.keys()]\n",
        "\n",
        "    df_tf = pd.DataFrame(df_tf_data, index=list(docs_content.keys()))\n",
        "\n",
        "    return df_tf, final_vocab\n",
        "\n",
        "# TF-IDF Calculation Functions (same as before)\n",
        "def calculate_log_tf(tf_matrix: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Calculates Log Term Frequency: log(1 + TF)\"\"\"\n",
        "    return np.log1p(tf_matrix)\n",
        "\n",
        "def calculate_idf(tf_matrix: pd.DataFrame, N: int) -> pd.Series:\n",
        "    \"\"\"Calculates Inverse Document Frequency: log(N / DF)\"\"\"\n",
        "    df = (tf_matrix > 0).sum(axis=0)\n",
        "    idf = np.log(N / df)\n",
        "    return idf\n",
        "\n",
        "def calculate_tfidf_scores(log_tf_matrix: pd.DataFrame, idf_series: pd.Series) -> pd.DataFrame:\n",
        "    \"\"\"Calculates TF-IDF: log_tf * idf\"\"\"\n",
        "    return log_tf_matrix * idf_series\n",
        "\n",
        "def calculate_query_score(tfidf_matrix: pd.DataFrame, query_terms: List[str]) -> pd.Series:\n",
        "    \"\"\"Calculates the Document Score (Sum of TF-IDF for query terms, assuming Query TF = 1).\"\"\"\n",
        "    relevant_terms = [t for t in query_terms if t in tfidf_matrix.columns]\n",
        "\n",
        "    if not relevant_terms:\n",
        "        print(\"Warning: None of the query terms were found in the calculated vocabulary.\")\n",
        "        return pd.Series(0.0, index=tfidf_matrix.index)\n",
        "\n",
        "    tfidf_query_subset = tfidf_matrix.loc[:, relevant_terms]\n",
        "    scores = tfidf_query_subset.sum(axis=1)\n",
        "\n",
        "    return scores\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "\n",
        "# 1. Load Documents\n",
        "DOCUMENTS_CONTENT = load_documents_from_disk(DOCUMENTS_DIR)\n",
        "N = len(DOCUMENTS_CONTENT)\n",
        "\n",
        "if N > 0:\n",
        "    # 2. Build the Raw TF Matrix\n",
        "    df_tf_raw, calculated_vocabulary = build_tf_matrix(\n",
        "        DOCUMENTS_CONTENT,\n",
        "        vocabulary_size=VOCABULARY_SIZE,\n",
        "        query_terms=QUERY_TERMS\n",
        "    )\n",
        "\n",
        "    # 3. Perform TF-IDF Calculations\n",
        "    df_log_tf = calculate_log_tf(df_tf_raw)\n",
        "    idf_scores = calculate_idf(df_tf_raw, N)\n",
        "    df_tfidf = calculate_tfidf_scores(df_log_tf, idf_scores)\n",
        "    query_scores = calculate_query_score(df_tfidf, QUERY_TERMS)\n",
        "\n",
        "    # 4. Prepare Final Output\n",
        "    df_output = df_tfidf.copy()\n",
        "    df_output.insert(0, 'Query_Score', query_scores)\n",
        "    df_output_sorted = df_output.sort_values(by='Query_Score', ascending=False)\n",
        "\n",
        "\n",
        "    # --- 5. Presentation ---\n",
        "\n",
        "    print(\"## Input Table: Raw Term Frequency (TF)\")\n",
        "    print(f\"**Total Documents (N):** {N}\")\n",
        "    print(f\"**Query:** {' + '.join(QUERY_TERMS)}\")\n",
        "    print(f\"**Vocabulary (10 terms):** {calculated_vocabulary}\")\n",
        "    print(\"---\")\n",
        "    print(df_tf_raw)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "    print(\"## Intermediate Results: IDF Scores\")\n",
        "    print(\"IDF (Inverse Document Frequency): High scores indicate rare terms.\")\n",
        "    print(\"---\")\n",
        "    print(idf_scores.sort_values(ascending=False))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "    print(\"## Final Output Table: Log TF-IDF and Query Score\")\n",
        "    print(\"Table is sorted by 'Query_Score' (relevance).\")\n",
        "    print(\"---\")\n",
        "    print(df_output_sorted)"
      ]
    }
  ]
}